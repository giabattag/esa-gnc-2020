\documentclass{esagnc}

% Abstract submission deadline: 27 February
% => TASF deadline: skipped
% ==> First draft deadline: ~10 February
% ===> Paolo deadline: ~6 February
% Paper submission deadline: 31 July
% => TASF deadline: ~15 July
% ==> First draft deadline: ~22 June
% ===> Paolo deadline: ~15 June

% Max: 25 words
\title{Development and optimization of a robust vision-based navigation algorithm for space-representative hardware}
\author[(1)]{Giacomo Battaglia}
\author[(3)]{Anthea Comellini}
\author[(4)]{Francesco Capolupo}
\author[(2)]{Paolo Panicucci}
\affil[(1)]{Politecnico di Milano, piazza Leonardo da Vinci, 32, 20133 Milan, giacomo.battaglia@polimi.it}
\affil[(2)]{Politecnico di Milano, piazza Leonardo da Vinci, 32, 20133 Milan, paolo.panicucci@polimi.it}
\affil[(3)]{Thales Alenia Space, Cannes, 06150, France, anthea.comellini@thalesaleniaspace.com}
\affil[(4)]{European Space Agency, Noordwijk, 2200 AG, Netherlands, francesco.capolupo@esa.int}

\footauth{G. Battaglia, A. Comellini, F. Capolupo, P. Panicucci}

\begin{document}

\maketitle

% Max: 500 words
% 489 words
\begin{abstract}
    Close proximity operations with non-cooperative targets represent a very challenging scenario for the GNC system. 
    Indeed, as the target does not give any information about its state, the chaser must rely solely on visual information to determine the relative pose (i.e., position, attitude) of the target with respect to the chaser. 
    That visual information is retrieved through the use of a monocular camera, chosen for its advantageous properties in terms of power, volume and mass budget in GNC system design. 
    The vision-based navigation pipeline is composed by two blocks: the image processing, which extracts contextual information from the input images to get useful data for the pose estimation task, and the navigation filter, which refines pose (and pose derivatives) estimation using time dependencies.
    This entire algorithm shall run on space hardware. Hence, the design and development chain shall be adapted to enable support in constraining environments in terms of computational speed and memory availability. 
    Moreover, when using an AI-based image processing, much attention shall be made into the deployment process of the network.
    This article presents the optimization and deployment of an AI-based vision-based navigation algorithm on representative space hardware. 
    The navigation pipeline is tailored to be robust to uncertainties in center-of-mass and inertia properties of the target, both on the image processing and the navigation filter. 
    The former is trained on a novel dataset, rendered with the high-fidelity SpiCam renderer by Thales Alenia Space, featuring a variable-shape target. The latter includes the uncertainties in the center of mass and the inertia of the target in a Schmidt-Kalman framework.
    Before full processor-in-the-loop validation, a preliminary assessment of the algorithm performance on hardware is de-risked in both the image processing and the navigation filter, outlining the different orders of magnitude of the latency performance in various devices.
    Then, the full algorithm pipeline is implemented for open-loop processor-in-the-loop testing, using representative boards for the two functional blocks.
    First, the AI-related part of the algorithm, i.e., the image processing, is embedded in specialized hardware of different type, namely CPU, GPU or FPGA boards.
    To do that, dedicated optimizations are performed, such as quantization and graph optimizations, depending on the target hardware.
    Secondly, the navigation filter is run on a single-core GR740 (LEON4) board, using single-precision arithmetics, which represents common current onboard computer hardware in space systems. 
    Depending on the performance of the platform, the pipeline is adapted in terms of navigation filter tuning and measurement frequency.
    For instance, lower accuracy is expected after 8-bit neural network quantization for FPGA, and a much lower measurement frequency is envisioned for the full-CPU implementation compared to GPU and FPGA.
    This analysis is performed on a synthetic testing trajectory rendered with the SpiCam tool, consisting of a close inspection relative orbit around a non-cooperative tumbling target in LEO.
\end{abstract}

\section*{Acknowledgments}

This work was conducted under EISI Agreement No. 4000144147, within the Open Space Innovation Platform (OSIP), through the support of the European Space Agency (ESA) and Thales Alenia Space France (TAS-F).

\bibliographystyle{esacit}
\bibliography{references}

\end{document}
